# ML-Hacking-Presentation
Slides and links 2020-2021 presentation on hacking ML systems

## Links

### Articles
* [H2O.ai - Can Your Machine Learning Model Be Hacked?!](https://www.h2o.ai/blog/can-your-machine-learning-model-be-hacked/)
* [Towards Datascience - Hacking Super Intelligence](https://towardsdatascience.com/hacking-super-intelligence-af5fe1fe6e26)
* [DZone - Security Attacks: Analysis of Machine Learning Models](https://dzone.com/articles/security-attacks-analysis-of-machine-learning-mode)
* [Skylight Cyber - Cylance, I Kill You!](https://skylightcyber.com/2019/07/18/cylance-i-kill-you/) A cybersecurity report on hacking an ML powered computer security product.
* [Asilomar AI Principles](https://futureoflife.org/ai-principles/?cn-reloaded=1) A set of ethical guidelines for AI researchers and practitioners.
* [Privacy and machine learning: two unexpected allies?](http://www.cleverhans.io/privacy/2018/04/29/privacy-and-machine-learning.html)
* [Cyberattacks against machine learning systems are more common than you think](https://www.microsoft.com/security/blog/2020/10/22/cyberattacks-against-machine-learning-systems-are-more-common-than-you-think/)
### Papers
* [Barreno et al. - The security of machine learning](https://people.eecs.berkeley.edu/~adj/publications/paper-files/SecML-MLJ2010.pdf) Defines a framework for classifying attacks on ML systems. Good place to start.
* [Goodfellow et al. - Explaining and Harnessing Adversarial Examples](https://arxiv.org/pdf/1412.6572.pdf) Introduces the Fast Gradient Sign Method for generating adversarial examples.
* [Papernot et al. - Practical Black-Box Attacks against Machine Learning](https://arxiv.org/abs/1602.02697)
* [Papernot et al. - Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples](https://arxiv.org/abs/1605.07277)
* [Madry et al. - Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/pdf/1706.06083.pdf)
* [Tram√®r et al. - Stealing Machine Learning models via prediction APIs](https://arxiv.org/pdf/1609.02943.pdf)
* [Chen et al. - Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning](https://arxiv.org/pdf/1712.05526.pdf)
* [N. Papernot - A Marauder's Map of Security and Privacy in Machine Learning](https://arxiv.org/abs/1811.01134)

### DEFCON Talks
* [GTKlondike Hacking with Skynet - How AI is Empowering Adversaries](https://www.youtube.com/watch?v=1ATu9tC4b04)
* [Vahid Behdazan - Security Challenges in Deep Reinforcement Learning](https://www.youtube.com/watch?v=u5SPg9XDqGM)
* [Richard Harang - A Tutorial on Hacking Facial Recognition Systems](https://www.youtube.com/watch?v=b-lkYtprgVw)
* [comathematician - Hyperlocal Drift Detection with Goko](https://www.youtube.com/watch?v=-F-eCJHSw9s) A method to potentially detect exploratory attacks by using the KL divergence and Cover Trees.
### Software
* [CleverHans](https://github.com/tensorflow/cleverhans) A set of benchmarks to test vulnerability to Adversarial Examples
### Courses
* [Infosec Institute - Hacking Machine Learning](https://www.infosecinstitute.com/skills/courses/hacking-machine-learning/) An online course on attacks on ML systems.